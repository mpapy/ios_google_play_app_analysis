{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I want to import all packages that I need for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import kagglehub\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to analyse the dataset to see, how the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/marekpapay/.cache/kagglehub/datasets/lava18/google-play-store-apps/versions/6\n",
      "Path to dataset files: /Users/marekpapay/.cache/kagglehub/datasets/ramamet4/app-store-apple-data-set-10k-apps/versions/7\n"
     ]
    }
   ],
   "source": [
    "path_google_dataset = kagglehub.dataset_download(\"lava18/google-play-store-apps\")\n",
    "print(\"Path to dataset files:\", path_google_dataset)\n",
    "path_ios_dataset = kagglehub.dataset_download(\"ramamet4/app-store-apple-data-set-10k-apps\")\n",
    "print(\"Path to dataset files:\", path_ios_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(dataset, start, end, rows_and_columns=False):\n",
    "    dataset_slice = dataset[start:end]    \n",
    "    for row in dataset_slice:\n",
    "        print(row)\n",
    "        print('\\n') # adds a new (empty) line after each row\n",
    "\n",
    "    if rows_and_columns:\n",
    "        print('Number of rows:', len(dataset))\n",
    "        print('Number of columns:', len(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will open the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_dataset_path = \"/Users/marekpapay/.cache/kagglehub/datasets/lava18/google-play-store-apps/versions/6/googleplaystore.csv\" \n",
    "apple_dataset_path = \"/Users/marekpapay/.cache/kagglehub/datasets/ramamet4/app-store-apple-data-set-10k-apps/versions/7/AppleStore.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(apple_dataset_path, newline='') as ios_file: \n",
    "    dataset_ios = list(csv.reader(ios_file)) \n",
    "with open(google_dataset_path, newline='') as google_file: \n",
    "    dataset_google = list(csv.reader(google_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset_google:\n",
    "    print(x[7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to analyse it and see the headers of the datasets and number of rows and number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'id', 'track_name', 'size_bytes', 'currency', 'price', 'rating_count_tot', 'rating_count_ver', 'user_rating', 'user_rating_ver', 'ver', 'cont_rating', 'prime_genre', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic']\n",
      "\n",
      "\n",
      "Number of rows: 7198\n",
      "Number of columns: 17\n",
      "\n",
      "\n",
      "['App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver']\n",
      "\n",
      "\n",
      "Number of rows: 10842\n",
      "Number of columns: 13\n"
     ]
    }
   ],
   "source": [
    "explore_data(dataset_ios, 0, 1, True)\n",
    "print('\\n')\n",
    "explore_data(dataset_google, 0, 1, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below, I want to print the most important columns, that I will focus on in the future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple:track_name rating_count_tot rating_count_ver user_rating user_rating_ver\n",
      "Google:Category Rating Type\n"
     ]
    }
   ],
   "source": [
    "print('Apple:' 'track_name', 'rating_count_tot', 'rating_count_ver', 'user_rating',\n",
    "     'user_rating_ver')\n",
    "print('Google:' 'Category','Rating', 'Type',  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will write a function that deletes rows, that are wrong in the dataset. Those rows are said to be wrong in the forum of the dataset in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_wrong_rows(dataset: list, index_number: int, start_number: int):\n",
    "    #I want to put index for all the rows\n",
    "    for index, row in enumerate(dataset, start = index_number):\n",
    "        #I want to see if there is a match\n",
    "        if index == index_number:\n",
    "            #delete the row\n",
    "            del dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_wrong_rows(dataset_google, 10473, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are apps that are duplicated, I need to create a function, that will delete all the duplicities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_apps(dataset: list, app_name: int, newest_app: int):\n",
    "    duplicate_apps_google = []\n",
    "    unique_apps_google = []\n",
    "\n",
    "    # Identify duplicate and unique apps\n",
    "    for app in dataset:\n",
    "        name = app[app_name]\n",
    "        if name in [app[app_name] for app in unique_apps_google]:\n",
    "            duplicate_apps_google.append(app)\n",
    "        else:\n",
    "            unique_apps_google.append(app)  \n",
    "\n",
    "    # Find the newest app version in duplicates\n",
    "    clear_newest_App_in_Dataset = {}\n",
    "\n",
    "    for highest_app in duplicate_apps_google:\n",
    "        app = str(highest_app[app_name])\n",
    "        number_app = int(highest_app[newest_app])\n",
    "        \n",
    "        if app not in clear_newest_App_in_Dataset:\n",
    "            clear_newest_App_in_Dataset[app] = number_app\n",
    "        \n",
    "        elif clear_newest_App_in_Dataset[app] < number_app:\n",
    "            clear_newest_App_in_Dataset[app] = number_app\n",
    "\n",
    "    # Create a list of unique apps with the newest version\n",
    "    for index, app in enumerate(dataset):\n",
    "        application = app[app_name]\n",
    "        number_app = app[newest_app]\n",
    "\n",
    "        for key, value in clear_newest_App_in_Dataset.items():\n",
    "\n",
    "            if application == key and number_app == value and number_app not in (dataset[app_name]):\n",
    "                unique_apps_google.append(app)\n",
    "    return unique_apps_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9660\n",
      "7196\n"
     ]
    }
   ],
   "source": [
    "google_dataset = process_apps(dataset_google, 0, 3)\n",
    "dataset_ios = process_apps(dataset_ios, 2, 3)\n",
    "print(len(google_dataset))\n",
    "print(len(dataset_ios))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep only the apps that are written in english, all others want to delete. We want to keep only ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_english_apps(dataset: list, index: int):\n",
    "    english_apps = []\n",
    "    \n",
    "    for row in dataset:\n",
    "        word = row[index]  # Extract the word\n",
    "        # Check if all characters are English and the app name is not already in the list\n",
    "        if all(ord(letter) < 128 for letter in word) and row not in english_apps:\n",
    "            english_apps.append(row)\n",
    "    \n",
    "    return english_apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_dataset = delete_non_english_apps(google_dataset, 0)\n",
    "ios_dataset = delete_non_english_apps(dataset_ios, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to isolate it only on Free Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_free_apps(dataset: str, index: int):\n",
    "    free_apps = []\n",
    "    non_free_apps = []\n",
    "    for row in dataset:\n",
    "        if row[index] == 0:\n",
    "            free_apps.append(row)\n",
    "        else:\n",
    "            non_free_apps.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_apps_ios = \n",
    "paid_apps_ios =\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
